{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a Naive Bayes Spam Classifier using Sci - kit. \n",
    "This is different from using NLTK of Python. Typically NTLK is used for creating the Feature Vector. Here we will use Scikit CountVectoriser for creating a Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First start reading files\n",
    "\n",
    "import os\n",
    "import  io\n",
    "from pandas import DataFrame\n",
    "\n",
    "def read_files(path):\n",
    "  for  root,dirname, filenames in  os.walk(path):\n",
    "    for filename in filenames:\n",
    "        path = os.path.join(root, filename)        \n",
    "        inBody = False\n",
    "        lines = []\n",
    "        f = io.open(path, 'r', encoding='latin1')\n",
    "        for line in f:\n",
    "            if inBody:\n",
    "                lines.append(line)\n",
    "            elif line=='\\n':\n",
    "                inBody=True\n",
    "        f.close()\n",
    "        message = '\\n'.join(lines)\n",
    "        \n",
    "        yield path, message\n",
    "        \n",
    "#Create a Df by reading th filess\n",
    "def create_df_from_directory(path, clasification):\n",
    "    rows = []\n",
    "    index = []\n",
    "    for filename, data in read_files(path):\n",
    "        rows.append({'message': data, 'classification': clasification})\n",
    "        index.append(filename)\n",
    "    return DataFrame(rows, index=index)\n",
    "    \n",
    "data = DataFrame({'message' :[], 'classification':[]} )        \n",
    "data = data.append(create_df_from_directory('C:\\\\Users\\\\212412835\\\\Documents\\\\Notebooks\\\\emails\\\\spam', 'spam'))\n",
    "data = data.append(create_df_from_directory('C:\\Users\\212412835\\Documents\\Notebooks\\emails\\ham', 'ham'))        \n",
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C:\\Users\\212412835\\Documents\\Notebooks\\emails\\spam\\00001.7848dde101aa985090474a91ec93fcf0</th>\n",
       "      <td>spam</td>\n",
       "      <td>&lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C:\\Users\\212412835\\Documents\\Notebooks\\emails\\spam\\00002.d94f1b97e48ed3b553b3508d116e6a09</th>\n",
       "      <td>spam</td>\n",
       "      <td>1) Fight The Risk of Cancer!\\n\\nhttp://www.adc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C:\\Users\\212412835\\Documents\\Notebooks\\emails\\spam\\00003.2ee33bc6eacdb11f38d052c44819ba6c</th>\n",
       "      <td>spam</td>\n",
       "      <td>1) Fight The Risk of Cancer!\\n\\nhttp://www.adc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C:\\Users\\212412835\\Documents\\Notebooks\\emails\\spam\\00004.eac8de8d759b7e74154f142194282724</th>\n",
       "      <td>spam</td>\n",
       "      <td>##############################################...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C:\\Users\\212412835\\Documents\\Notebooks\\emails\\spam\\00005.57696a39d7d84318ce497886896bf90d</th>\n",
       "      <td>spam</td>\n",
       "      <td>I thought you might like these:\\n\\n1) Slim Dow...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   classification  \\\n",
       "C:\\Users\\212412835\\Documents\\Notebooks\\emails\\s...           spam   \n",
       "C:\\Users\\212412835\\Documents\\Notebooks\\emails\\s...           spam   \n",
       "C:\\Users\\212412835\\Documents\\Notebooks\\emails\\s...           spam   \n",
       "C:\\Users\\212412835\\Documents\\Notebooks\\emails\\s...           spam   \n",
       "C:\\Users\\212412835\\Documents\\Notebooks\\emails\\s...           spam   \n",
       "\n",
       "                                                                                              message  \n",
       "C:\\Users\\212412835\\Documents\\Notebooks\\emails\\s...  <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.0 Tr...  \n",
       "C:\\Users\\212412835\\Documents\\Notebooks\\emails\\s...  1) Fight The Risk of Cancer!\\n\\nhttp://www.adc...  \n",
       "C:\\Users\\212412835\\Documents\\Notebooks\\emails\\s...  1) Fight The Risk of Cancer!\\n\\nhttp://www.adc...  \n",
       "C:\\Users\\212412835\\Documents\\Notebooks\\emails\\s...  ##############################################...  \n",
       "C:\\Users\\212412835\\Documents\\Notebooks\\emails\\s...  I thought you might like these:\\n\\n1) Slim Dow...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 12496)\t1\n",
      "  (0, 17635)\t5\n",
      "  (0, 27488)\t1\n",
      "  (0, 35042)\t1\n",
      "  (0, 12977)\t1\n",
      "  (0, 32767)\t1\n",
      "  (0, 13775)\t1\n",
      "  (0, 17025)\t2\n",
      "  (0, 22954)\t2\n",
      "  (0, 10959)\t2\n",
      "  (0, 2233)\t28\n",
      "  (0, 32210)\t1\n",
      "  (0, 10181)\t1\n",
      "  (0, 2421)\t1\n",
      "  (0, 660)\t1\n",
      "  (0, 17643)\t2\n",
      "  (0, 13991)\t1\n",
      "  (0, 2336)\t1\n",
      "  (0, 37605)\t1\n",
      "  (0, 23530)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 1424)\t1\n",
      "  (0, 512)\t1\n",
      "  (0, 23930)\t1\n",
      "  (0, 2351)\t1\n",
      "  :\t:\n",
      "  (499, 39360)\t1\n",
      "  (499, 38874)\t1\n",
      "  (499, 39347)\t1\n",
      "  (499, 39115)\t1\n",
      "  (499, 23802)\t1\n",
      "  (499, 16712)\t2\n",
      "  (499, 39332)\t2\n",
      "  (499, 39331)\t2\n",
      "  (499, 39507)\t2\n",
      "  (499, 38872)\t2\n",
      "  (499, 39386)\t1\n",
      "  (499, 39452)\t1\n",
      "  (499, 39283)\t1\n",
      "  (499, 38900)\t1\n",
      "  (499, 38955)\t1\n",
      "  (499, 38883)\t1\n",
      "  (499, 39328)\t1\n",
      "  (499, 38958)\t1\n",
      "  (499, 39468)\t1\n",
      "  (499, 39090)\t1\n",
      "  (499, 38957)\t1\n",
      "  (499, 38823)\t1\n",
      "  (499, 39503)\t1\n",
      "  (499, 22391)\t1\n",
      "  (499, 29868)\t1\n"
     ]
    }
   ],
   "source": [
    "#Now lets pull in a Count Vectoriser, and split each message to a list of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "counts = vectorizer.fit_transform(data['message'].values)\n",
    "print (counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Throw this to MultinomialNB\n",
    "classifier = MultinomialNB()\n",
    "targets = data['classification'].values\n",
    "classifier.fit(counts, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['spam', 'spam'], \n",
       "      dtype='|S4')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = ['Free Viagra now!!!', \"Hi Bob, how about a game of golf tomorrow?\"]\n",
    "example_counts = vectorizer.transform(examples)\n",
    "predictions = classifier.predict(example_counts)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
